import pandas as pd
import os
import numpy as np
from actipy import read_device
from config import project_config as config


def read_AWS_labels(path, subject_id):
    aws_df = pd.read_csv(f'{path}/mesa-sleep-{subject_id:04d}_Date_time.csv')

    aws_cols = ['Date_time', 'Sleep/Wake', 'Interval Status']
    aws_df = aws_df[aws_cols]
    aws_df['Interval Status'] = aws_df['Interval Status'].map({'ACTIVE': 0, 'REST': 0, 'REST-S': 1})
    aws_df['Sleep/Wake'] = aws_df['Sleep/Wake'].map({1: 0, 0: 1})  # This one is recorded the "wrong" way
    aws_df = aws_df.rename({
        'Date_time': 'AWS time',
        'Sleep/Wake': 'AWS Sleep',
        'Interval Status': 'AWS Interval Status',
        }, axis=1)
    aws_df['AWS time'] = pd.to_datetime(aws_df['AWS time'])

    return aws_df


def read_PSG_labels(path, subject_id):
    labels_df = pd.read_csv(f'{path}/SDRI001_PSG_Sleep profile_{subject_id:03d}V4_N1.txt', skiprows=1, delimiter=';', header=None)
    labels_df = labels_df.rename({0: 'epoch_ts', 1: 'PSG Sleep'}, axis=1)
    labels_df['PSG Sleep'] = labels_df['PSG Sleep'].str.strip()  # remove extra spaces
    
    all_label_values = ['Wake', 'N1', 'N2', 'A', 'N3', 'REM', 'Artefact']
    known_labels = labels_df['PSG Sleep'].isin(all_label_values)
    assert (known_labels.all()), f"Encountered unknown label(s): {pd.unique(labels_df.loc[~known_labels, 'PSG Sleep'])}"

    missing_fltr = labels_df['PSG Sleep'].isin(['A', 'Artefact'])
    missing_pct = missing_fltr.mean()
    # print(f'Dropping missing epochs ({round(missing_pct * 100, 2)}%)')
    labels_df = labels_df[~missing_fltr]
    
    labels_df['epoch_ts'] = labels_df['epoch_ts'].apply(lambda ts: ts.split(',')[0])  # There is a weird ",000" at the end of timestamps
    
    labels_df['epoch_ts'] = labels_df['epoch_ts'].str.strip()
    labels_df['epoch_ts'] = pd.to_datetime(labels_df['epoch_ts'], dayfirst=True)

    labels_df['PSG Sleep'] = labels_df['PSG Sleep'].map(lambda l: 0 if l == 'Wake' else 1)
    labels_df['PSG Sleep'] = labels_df['PSG Sleep'].astype(np.float32)  # TF requires labels to be float

    return labels_df


def read_parquet_AX3_epochs(path, subject_id, round_timestamps):
    """
    This function reads Pickle files that contain one record per epoch
    Each key (column) will be a list of values. e.g., X contains 3000 numbers per epoch/record
    These files should be the ones generated by the `read_AX3_cwa` function
    which converts raw cwa files into these pickles.
    
    Note: The reason for CWA to PKL and then PKL to Tensorflow is that reading the original
        CWA files is slower than reading Pickles. But Pickles are faster to read
    """
    filename = f'{path}/AX3_sub_{subject_id:02d}.parquet'
    subject_data = pd.read_parquet(filename)

    subject_data = subject_data[['subject_id', 'epoch_ts', 'X', 'Y', 'Z', 'Temp']]

    # subject_data = subject_data.rename({'Label': 'epoch_ts'}, axis=1)
    # subject_data['epoch_ts'] = subject_data['epoch_ts'].str.strip()
    # subject_data['epoch_ts'] = pd.to_datetime(subject_data['epoch_ts'])
    subject_data = subject_data.sort_values('epoch_ts')

    if round_timestamps:
        # Timestamps of PSG labels are at 0 and 30 seconds. We need to somehow align these timestamps with those of the labels
        subject_data['epoch_ts'] = subject_data['epoch_ts'].dt.floor('30s')  # Round down [0, 29] to 0 and [30, 59] to 30

    return subject_data


def read_AX3_cwa(path, subject_id, freq=config['AX3_freq'], seconds_per_epoch=config['seconds_per_epoch']):

    rows_per_epoch = freq * seconds_per_epoch

    cwa_files = [f for f in os.listdir(f'{path}') if f.endswith('.cwa')]
    subject_files = [filename for filename in cwa_files if filename.find(f'AX3_ALL_{subject_id:03d}') >= 0]

    subject_data = pd.DataFrame()
    for i, filename in enumerate(subject_files):
        print(f'File {i + 1}:')
        # Reading device data with non-wear detection turned off
        # non-wear detection is not relevant for labelled data, since ground truth is known
        # For unlabelled data (no PSG label) we will compare our epochs to the output of the other
        # toolbox (https://github.com/OxWearables/biobankAccelerometerAnalysis/tree/master)
        # and drop epochs that it doesn't predict for (i.e. it decides they are non-wear epochs)
        data_chunk, info = read_device(f'{path}/{filename}', resample_hz=freq, detect_nonwear=False)

        data_chunk = data_chunk.drop(columns=['light'])

        # Resetting the index has two functions:
        # 1) Time comes in as the index. We want it as a column.
        # 2) Ensures the index corresponds to row numbers for the next step
        data_chunk = data_chunk.reset_index()
        # We need to ensure the length of data is a multiple of `rows_per_epoch`
        # So we will trim the tail of the dataframe to make it so
        target_n_rows = len(data_chunk) - (len(data_chunk) % rows_per_epoch)
        data_chunk = data_chunk[data_chunk.index < target_n_rows]  # strictly less, because index is 0-based
        
        subject_data = pd.concat([subject_data, data_chunk])

    # Clean and convert the time column
    # subject_data['time'] = subject_data['time'].apply(lambda s: " ".join(s.split(" ")[:2])) # Remove time zone string
    # subject_data['time'] = pd.to_datetime(subject_data['time']).dt.tz_localize(None)  # Remove time zone
    subject_data = subject_data.sort_values('time')

    subject_data.insert(0, 'subject_id', subject_id)  # This is an in place operation
    
    # This uses index in place of row number. Since we just sorted the data, reseting the index ensures it actually is row number.
    # epoch_id is not used in this function but other functions use it later.
    subject_data.insert(1, 'epoch_id', subject_data.reset_index(drop=True).index // rows_per_epoch)

    return subject_data


def process_AX3_raw_data(df, round_timestamps, normalise_columns=[]):

    # Normalising features
    for col in normalise_columns:
        # A very tiny number of observations are Nan in a handful of files
        # In practice they don't cause an issue. The following mean and std functions
        # ignore NaNs
        df[col] = (df[col] - df[col].mean()) / df[col].std()
        df[col] = df[col].round(4)  # original data also has 4 decimal places
    
    # Collecting each epoch into a single row
    df = df.groupby('epoch_id').agg(
        subject_id=('subject_id', 'min'),
        epoch_ts=('epoch_ts', 'min'),
        X=('X', list),
        Y=('Y', list),
        Z=('Z', list),
        Temp=('Temp', list),
        ).reset_index(drop=True)
    
    if round_timestamps:
        # Timestamps of PSG labels are at 0 and 30 seconds. We need to somehow align these timestamps with those of the labels
        df['epoch_ts'] = df['epoch_ts'].dt.floor('30s')  # Round down [0, 29] to 0 and [30, 59] to 30

    return df


def read_sleep_dairies(path, include_naps):
    raise NotImplementedError("These sleep diary files are old. Use v2")
    sleep_diary_df = pd.DataFrame()
    for filename in [f for f in os.listdir(path) if f.endswith('csv')]:
        if filename.find('nap') >= 0:
            continue
        df = pd.read_csv(f'{path}/{filename}')
        sleep_diary_df = pd.concat([sleep_diary_df, df])

    # reading the extra nap diaries
    if include_naps:
        nap_df = pd.read_csv(f'{path}/SRCDRI001_Sleep Diary 019-036_nap.csv')
        nap_df = nap_df.rename(columns={
            'date_startnap': 'date_gotosleep',
            'date_endnap': 'date_finalawake',
            'nap_start': 'gotosleep',
            'nap_end': 'finalawake'   
        }).drop(columns=['nap times'])
        sleep_diary_df = pd.concat([sleep_diary_df, nap_df])
    
    sleep_diary_df = sleep_diary_df.sort_values(['participantNo', 'date_gotosleep']).reset_index(drop=True)

    sleep_diary_df['sleep_start'] = pd.to_datetime(sleep_diary_df['date_gotosleep'] + ' ' + sleep_diary_df['gotosleep'])
    sleep_diary_df['sleep_end'] = pd.to_datetime(sleep_diary_df['date_finalawake'] + ' ' + sleep_diary_df['finalawake'])
    sleep_diary_df = sleep_diary_df[['participantNo', 'sleep_start', 'sleep_end']]
    
    return sleep_diary_df


def read_sleep_dairies_v2(path, include_naps):

    def clean_time_column(time_col):  # This is some messy data (times as strings) cleaning
        def strip_junk_chars(s):
            colon_count = len([ch for ch in s if ch == ':'])
            if colon_count == 2:  # if there are two ":"s it's a time, clean it
                return ''.join([ch for ch in s if ch.isnumeric() or ch == ':'])
            else:  # if not drop it
                return np.nan
        
        return time_col.astype(str).apply(strip_junk_chars)
    
    # There are two files with different formats for two groups of subjects

    # # # # # File 1: ids 1-18

    d_1 = pd.read_excel('data/Sleep diaries/SRC_DRI_001_SleepDiary_All_v5.0_20JAN2021.xlsx', sheet_name='Tot_sample')

    d_1 = d_1.sort_values(['participantNo', 'date_gotosleep']).reset_index(drop=True)
    
    d_1['lights_off'] = pd.to_datetime(d_1['date_gotosleep'].astype(str) + ' ' + d_1['gotosleep'].astype(str))
    d_1['lights_on'] = pd.to_datetime(d_1['date_finalawake'].astype(str) + ' ' + d_1['finalawake'].astype(str))
    d_1 = d_1[['participantNo', 'lights_off', 'lights_on']]
    d_1 = d_1.rename({'participantNo': 'subject_id'}, axis=1)

    # # # # # File 2: ids 19-36

    d_2 = pd.read_excel('data/Sleep diaries/SRCDRI001_Sleep Diary 019-036 DB.xlsx', header=1)

    # These two have not been implemented or used yet
    # What time did you get into bed?  -->  sleep start
    # What time did you get out of bed for the day?  -->  sleep end
    
    # What time did you try to go to sleep?  -->  substitute for lights out
    # What time was your final awakening?  -->  substitute for lights on
    
    lights_off_col = 'What time did you try to go to sleep?'
    lights_on_col = 'What time was your final awakening?'

    for col in [lights_off_col, lights_on_col]:
        d_2[col] = clean_time_column(d_2[col])  # Clean / drop missing time columns
        d_2 = d_2.dropna(subset=[col])  # The function above replaces unsalvageable time-string with NaNs

    # This file includes naps, which we may not want
    nap_indicator_col = 'Why did you take a nap?'
    not_a_nap_fltr = d_2[nap_indicator_col].isna() | d_2[nap_indicator_col].isin(['x', 'X', 0, '-'])

    if not include_naps:  # Remove naps if not wanted
        d_2 = d_2[not_a_nap_fltr]  # only keep non-nap episodes
    else:
        d_2['is_nap'] = (~not_a_nap_fltr).astype(int)

    # This file (subjects 19-36) has a single date per row
    # So we need to infer sleep_start and sleep_end dates
    # Sleep end date is always the given date + 1, becuase even if sleep start after midnight this date will still show the day before
    # Sleep start is the same only if, time of sleep is before 00:00:00
    d_2['today'] = d_2['Date on Diary']
    d_2['tomorrow'] = d_2['Date on Diary'] + np.timedelta64(1, 'D')

    # # # Date for sleep end
    # date + time to make a complete datetime
    d_2[lights_on_col] = pd.to_datetime(d_2['tomorrow'].astype(str) + ' ' + d_2[lights_on_col].astype(str))

    # # # Date for sleep start. depends on wether sleep start before or after midnight
    d_2['lights_off_hour'] = d_2[lights_off_col].astype(str).apply(lambda s: s.split(':')[0]).astype(int)
    
    # The date is assigned based on wether sleep starts before or after midnight (hour < 12)
    d_2['lights_off_date'] = d_2['today'].where(d_2['lights_off_hour'] > 12, d_2['tomorrow'])  
    # date + time to make a complete datetime
    d_2[lights_off_col] = pd.to_datetime(d_2['lights_off_date'].astype(str) + ' ' + d_2[lights_off_col])

    # # # Done
    
    cols_to_keep = ['Participant No.', lights_off_col, lights_on_col]
    if include_naps:
        cols_to_keep += ['is_nap']

    d_2 = d_2[cols_to_keep]

    d_2 = d_2.rename({
        'Participant No.': 'subject_id',
        lights_off_col: 'lights_off',
        lights_on_col: 'lights_on',
    }, axis=1)
    
    diary = pd.concat([d_1, d_2])
    
    if include_naps:
        diary['is_nap'] = diary['is_nap'].fillna(0)  # d_1 didn't have this column because file 1 doesn't have naps

    return diary
