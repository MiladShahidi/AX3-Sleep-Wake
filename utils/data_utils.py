from datetime import datetime
import pandas as pd
import re
import os
import pickle
import numpy as np
from actipy import read_device
from config import project_config as config


def read_AWS_labels(path, subject_id):
    aws_df = pd.read_csv(f'{path}/mesa-sleep-{subject_id:04d}_Date_time.csv')

    aws_cols = ['Date_time', 'Sleep/Wake', 'Interval Status']
    aws_df = aws_df[aws_cols]
    aws_df['Interval Status'] = aws_df['Interval Status'].map({'ACTIVE': 0, 'REST': 0, 'REST-S': 1})
    aws_df['Sleep/Wake'] = aws_df['Sleep/Wake'].map({1: 0, 0: 1})  # This one is recorded the "wrong" way
    aws_df = aws_df.rename({
        'Date_time': 'AWS time',
        'Sleep/Wake': 'AWS Sleep',
        'Interval Status': 'AWS Interval Status',
        }, axis=1)
    aws_df['AWS time'] = pd.to_datetime(aws_df['AWS time'])

    return aws_df


def read_PSG_labels(path, subject_id):
    labels_df = pd.read_csv(f'{path}/SDRI001_PSG_Sleep profile_{subject_id:03d}V4_N1.txt', skiprows=1, delimiter=';', header=None)
    labels_df = labels_df.rename({0: 'epoch_ts', 1: 'PSG Sleep'}, axis=1)
    labels_df['PSG Sleep'] = labels_df['PSG Sleep'].str.strip()  # remove extra spaces
    
    all_label_values = ['Wake', 'N1', 'N2', 'A', 'N3', 'REM', 'Artefact']
    known_labels = labels_df['PSG Sleep'].isin(all_label_values)
    assert (known_labels.all()), f"Encountered unknown label(s): {pd.unique(labels_df.loc[~known_labels, 'PSG Sleep'])}"

    missing_fltr = labels_df['PSG Sleep'].isin(['A', 'Artefact'])
    missing_pct = missing_fltr.mean()
    # print(f'Dropping missing epochs ({round(missing_pct * 100, 2)}%)')
    labels_df = labels_df[~missing_fltr]
    
    labels_df['epoch_ts'] = labels_df['epoch_ts'].apply(lambda ts: ts.split(',')[0])  # There is a weird ",000" at the end of timestamps
    
    labels_df['epoch_ts'] = labels_df['epoch_ts'].str.strip()
    labels_df['epoch_ts'] = pd.to_datetime(labels_df['epoch_ts'], dayfirst=True)

    labels_df['PSG Sleep'] = labels_df['PSG Sleep'].map(lambda l: 0 if l == 'Wake' else 1)
    labels_df['PSG Sleep'] = labels_df['PSG Sleep'].astype(np.float32)  # TF requires labels to be float

    return labels_df


def read_sleep_dairies(path):
    sleep_diary_df = pd.DataFrame()
    for filename in [f for f in os.listdir(path) if f.endswith('csv')]:
        if filename.find('nap') >= 0:
            continue
        df = pd.read_csv(f'{path}/{filename}')
        sleep_diary_df = pd.concat([sleep_diary_df, df])

    # reading the extra nap diaries
    nap_df = pd.read_csv(f'{path}/SRCDRI001_Sleep Diary 019-036_nap.csv')
    nap_df = nap_df.rename(columns={
        'date_startnap': 'date_gotosleep',
        'date_endnap': 'date_finalawake',
        'nap_start': 'gotosleep',
        'nap_end': 'finalawake'   
    }).drop(columns=['nap times'])
    sleep_diary_df = pd.concat([sleep_diary_df, nap_df])
    sleep_diary_df = sleep_diary_df.sort_values(['participantNo', 'date_gotosleep']).reset_index(drop=True)

    sleep_diary_df['sleep_start'] = pd.to_datetime(sleep_diary_df['date_gotosleep'] + ' ' + sleep_diary_df['gotosleep'])
    sleep_diary_df['sleep_end'] = pd.to_datetime(sleep_diary_df['date_finalawake'] + ' ' + sleep_diary_df['finalawake'])
    sleep_diary_df = sleep_diary_df[['participantNo', 'sleep_start', 'sleep_end']]
    return sleep_diary_df


def read_parquet_AX3_epochs(path, subject_id, round_timestamps):
    """
    This function reads Pickle files that contain one record per epoch
    Each key (column) will be a list of values. e.g., X contains 3000 numbers per epoch/record
    These files should be the ones generated by the `read_AX3_cwa` function
    which converts raw cwa files into these pickles.
    
    Note: The reason for CWA to PKL and then PKL to Tensorflow is that reading the original
        CWA files is slower than reading Pickles. But Pickles are faster to read
    """
    filename = f'{path}/AX3_sub_{subject_id:02d}.parquet'
    subject_data = pd.read_parquet(filename)

    subject_data = subject_data[['subject_id', 'epoch_ts', 'X', 'Y', 'Z', 'Temp']]

    # subject_data = subject_data.rename({'Label': 'epoch_ts'}, axis=1)
    # subject_data['epoch_ts'] = subject_data['epoch_ts'].str.strip()
    # subject_data['epoch_ts'] = pd.to_datetime(subject_data['epoch_ts'])
    subject_data = subject_data.sort_values('epoch_ts')

    if round_timestamps:
        # Timestamps of PSG labels are at 0 and 30 seconds. We need to somehow align these timestamps with those of the labels
        subject_data['epoch_ts'] = subject_data['epoch_ts'].dt.floor('30s')  # Round down [0, 29] to 0 and [30, 59] to 30

    return subject_data


def read_AX3_cwa(path, subject_id, freq=config['AX3_freq'], seconds_per_epoch=config['seconds_per_epoch']):

    rows_per_epoch = freq * seconds_per_epoch

    cwa_files = [f for f in os.listdir(f'{path}') if f.endswith('.cwa')]
    subject_files = [filename for filename in cwa_files if filename.find(f'AX3_ALL_{subject_id:03d}') >= 0]

    subject_data = pd.DataFrame()
    for i, filename in enumerate(subject_files):
        print(f'File {i + 1}:')
        # Reading device data with non-wear detection turned off
        # non-wear detection is not relevant for labelled data, since ground truth is known
        # For unlabelled data (no PSG label) we will compare our epochs to the output of the other
        # toolbox (https://github.com/OxWearables/biobankAccelerometerAnalysis/tree/master)
        # and drop epochs that it doesn't predict for (i.e. it decides they are non-wear epochs)
        data_chunk, info = read_device(f'{path}/{filename}', resample_hz=freq, detect_nonwear=False)

        data_chunk = data_chunk.drop(columns=['light'])

        # Resetting the index has two functions:
        # 1) Time comes in as the index. We want it as a column.
        # 2) Ensures the index corresponds to row numbers for the next step
        data_chunk = data_chunk.reset_index()
        # We need to ensure the length of data is a multiple of `rows_per_epoch`
        # So we will trim the tail of the dataframe to make it so
        target_n_rows = len(data_chunk) - (len(data_chunk) % rows_per_epoch)
        data_chunk = data_chunk[data_chunk.index < target_n_rows]  # strictly less, because index is 0-based
        
        subject_data = pd.concat([subject_data, data_chunk])

    # Clean and convert the time column
    # subject_data['time'] = subject_data['time'].apply(lambda s: " ".join(s.split(" ")[:2])) # Remove time zone string
    # subject_data['time'] = pd.to_datetime(subject_data['time']).dt.tz_localize(None)  # Remove time zone
    subject_data = subject_data.sort_values('time')

    subject_data.insert(0, 'subject_id', subject_id)  # This is an in place operation
    
    # This uses index in place of row number. Since we just sorted the data, reseting the index ensures it actually is row number.
    # epoch_id is not used in this function but other functions use it later.
    subject_data.insert(1, 'epoch_id', subject_data.reset_index(drop=True).index // rows_per_epoch)

    return subject_data


def process_AX3_raw_data(df, round_timestamps, normalise_columns=[]):

    # Normalising features
    for col in normalise_columns:
        # A very tiny number of observations are Nan in a handful of files
        # In practice they don't cause an issue. The following mean and std functions
        # ignore NaNs
        df[col] = (df[col] - df[col].mean()) / df[col].std()
        df[col] = df[col].round(4)  # original data also has 4 decimal places
    
    # Collecting each epoch into a single row
    df = df.groupby('epoch_id').agg(
        subject_id=('subject_id', 'min'),
        epoch_ts=('epoch_ts', 'min'),
        X=('X', list),
        Y=('Y', list),
        Z=('Z', list),
        Temp=('Temp', list),
        ).reset_index(drop=True)
    
    if round_timestamps:
        # Timestamps of PSG labels are at 0 and 30 seconds. We need to somehow align these timestamps with those of the labels
        df['epoch_ts'] = df['epoch_ts'].dt.floor('30s')  # Round down [0, 29] to 0 and [30, 59] to 30

    return df


def read_sleep_dairies(path):
    sleep_diary_df = pd.DataFrame()
    for filename in [f for f in os.listdir(path) if f.endswith('csv')]:
        if filename.find('nap') >= 0:
            continue
        df = pd.read_csv(f'{path}/{filename}')
        sleep_diary_df = pd.concat([sleep_diary_df, df])

    # reading the extra nap diaries
    nap_df = pd.read_csv(f'{path}/SRCDRI001_Sleep Diary 019-036_nap.csv')
    nap_df = nap_df.rename(columns={
        'date_startnap': 'date_gotosleep',
        'date_endnap': 'date_finalawake',
        'nap_start': 'gotosleep',
        'nap_end': 'finalawake'   
    }).drop(columns=['nap times'])
    sleep_diary_df = pd.concat([sleep_diary_df, nap_df])
    sleep_diary_df = sleep_diary_df.sort_values(['participantNo', 'date_gotosleep']).reset_index(drop=True)

    sleep_diary_df['sleep_start'] = pd.to_datetime(sleep_diary_df['date_gotosleep'] + ' ' + sleep_diary_df['gotosleep'])
    sleep_diary_df['sleep_end'] = pd.to_datetime(sleep_diary_df['date_finalawake'] + ' ' + sleep_diary_df['finalawake'])
    sleep_diary_df = sleep_diary_df[['participantNo', 'sleep_start', 'sleep_end']]
    return sleep_diary_df