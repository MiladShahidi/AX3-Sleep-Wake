import pandas as pd
import re
import os
import pickle
import numpy as np
from actipy import read_device
from config import project_config as config


def read_PSG_labels(path, subject_id):
    labels_df = pd.read_csv(f'{path}/SDRI001_PSG_Sleep profile_{subject_id:03d}V4_N1.txt', skiprows=1, delimiter=';', header=None)
    labels_df = labels_df.rename({0: 'epoch_ts', 1: 'label'}, axis=1)
    labels_df['label'] = labels_df['label'].str.strip()  # remove extra spaces
    
    all_label_values = ['Wake', 'N1', 'N2', 'A', 'N3', 'REM', 'Artefact']
    known_labels = labels_df['label'].isin(all_label_values)
    assert (known_labels.all()), f"Encountered unknown label(s): {pd.unique(labels_df.loc[~known_labels, 'label'])}"

    missing_fltr = labels_df['label'].isin(['A', 'Artefact'])
    missing_pct = missing_fltr.mean()
    # print(f'Dropping missing epochs ({round(missing_pct * 100, 2)}%)')
    labels_df = labels_df[~missing_fltr]
    
    labels_df['epoch_ts'] = labels_df['epoch_ts'].apply(lambda ts: ts.split(',')[0])  # There is a weird ",000" at the end of timestamps
    
    labels_df['epoch_ts'] = labels_df['epoch_ts'].str.strip()
    labels_df['epoch_ts'] = pd.to_datetime(labels_df['epoch_ts'], dayfirst=True)

    labels_df['label'] = labels_df['label'].map(lambda l: 0 if l == 'Wake' else 1)
    labels_df['label'] = labels_df['label'].astype(np.float32)  # TF requires labels to be float

    return labels_df


def read_AX3_pkl_epoch(path, subject_id, round_timestamps):
    """
    This function reads Pickle files that contain one record per epoch
    Each key (column) will be a list of values. e.g., X contains 3000 numbers per epoch/record
    These files should be the ones generated by the `read_AX3_cwa` function
    which converts raw cwa files into these pickles.
    
    Note: The reason for CWA to PKL and then PKL to Tensorflow is that reading the original
        CWA files is slower than reading Pickles. But Pickles are faster to read
    """
    filename = f'{path}/AX3_sub_{subject_id:02d}.pkl'
    with open(filename, 'rb') as f:
        subject_data = pickle.load(f)

    subject_data = subject_data[['epoch_ts', 'X', 'Y', 'Z', 'Temp']]
    # subject_data = subject_data.rename({'Label': 'epoch_ts'}, axis=1)
    # subject_data['epoch_ts'] = subject_data['epoch_ts'].str.strip()
    # subject_data['epoch_ts'] = pd.to_datetime(subject_data['epoch_ts'])
    subject_data = subject_data.sort_values('epoch_ts')

    if round_timestamps:
        # Timestamps of PSG labels are at 0 and 30 seconds. We need to somehow align these timestamps with those of the labels
        subject_data['epoch_ts'] = subject_data['epoch_ts'].dt.floor('30s')  # Round down [0, 29] to 0 and [30, 59] to 30

    return subject_data


def read_AX3_cwa(path, subject_id, round_timestamps, freq=config['AX3_freq'], seconds_per_epoch=config['seconds_per_epoch']):

    rows_per_epoch = freq * seconds_per_epoch

    cwa_files = [f for f in os.listdir(f'{path}') if f.endswith('.cwa')]
    subject_files = [filename for filename in cwa_files if filename.find(f'AX3_ALL_{subject_id:03d}') >= 0]

    subject_data = pd.DataFrame()
    for filename in subject_files:
        # Reading device data with non-wear detection turned off
        # non-wear detection is not relevant for labelled data, since ground truth is known
        # For unlabelled data (no PSG label) we will compare our epochs to the output of the other
        # toolbox (https://github.com/OxWearables/biobankAccelerometerAnalysis/tree/master)
        # and drop epochs that it doesn't predict for (i.e. it decides they are non-wear epochs)
        data_chunk, info = read_device(f'{path}/{filename}', resample_hz=freq, detect_nonwear=False)

        data_chunk = data_chunk.drop(columns=['light'])

        # Resetting the index has two functions:
        # 1) Time comes in as the index. We want it as a column.
        # 2) Ensures the index corresponds to row numbers for the next step
        data_chunk = data_chunk.reset_index()
        # We need to ensure the length of data is a multiple of `rows_per_epoch`
        # So we will trim the tail of the dataframe to make it so
        target_n_rows = len(data_chunk) - (len(data_chunk) % rows_per_epoch)
        data_chunk = data_chunk[data_chunk.index < target_n_rows]  # strictly less, because index is 0-based
        
        subject_data = pd.concat([subject_data, data_chunk])

    # Clean and convert the time column
    # subject_data['time'] = subject_data['time'].apply(lambda s: " ".join(s.split(" ")[:2])) # Remove time zone string
    # subject_data['time'] = pd.to_datetime(subject_data['time']).dt.tz_localize(None)  # Remove time zone
    subject_data = subject_data.sort_values('time')

    # This uses index in place of row number. Since we just sorted the data, reseting the index ensures it actually is row number.
    subject_data['epoch_id'] = subject_data.reset_index(drop=True).index // rows_per_epoch

    subject_data = subject_data.groupby('epoch_id').agg(
        epoch_ts=('time', 'min'),
        X=('x', list),
        Y=('y', list),
        Z=('z', list),
        Temp=('temperature', list),
        ).reset_index(drop=True)
    
    if round_timestamps:
        # Timestamps of PSG labels are at 0 and 30 seconds. We need to somehow align these timestamps with those of the labels
        subject_data['epoch_ts'] = subject_data['epoch_ts'].dt.floor('30s')  # Round down [0, 29] to 0 and [30, 59] to 30

    subject_data.insert(0, 'subject_id', subject_id)  # This is an in place operation

    return subject_data